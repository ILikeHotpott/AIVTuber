"""
Realtime LangGraph chat engine â€”â€” å¸¦ ElasticSearch é•¿è®°å¿†æ£€ç´¢ + Unity å£åž‹åŒæ­¥ TTSã€‚
ä¾èµ–:
  â€¢ src/tts/tts_player.py      (æä¾› TTSPlayer / TTSConfig)
  â€¢ src/memory/long_term/...   (å¯é€‰)
  â€¢ src/prompt/templates/...   (ç³»ç»Ÿæç¤ºè¯)
  â€¢ src/user/user_manager.py   (æ–°å¢žï¼šç”¨æˆ·ç®¡ç†)
ä½œè€…: Yitong  Â· 2025-05-31
"""

from __future__ import annotations

import asyncio
import os
import re
import sqlite3
import threading
from datetime import datetime
from typing import Annotated, Any, Dict, List, Sequence, TypedDict, Optional

from dotenv import load_dotenv
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages

from src.tts.tts_player import TTSPlayer
from src.tts.tts_config import TTSConfig

from src.utils.path import find_project_root

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šç”¨æˆ·ç®¡ç†é›†æˆ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from src.danmaku.user.user_manager import UserManager

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optional project imports â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from src.memory.long_term.elastic_search import LongTermMemoryES
    from src.prompt.templates.general import general_settings_prompt_english
except ModuleNotFoundError:
    LongTermMemoryES = None  # noqa: N816
    general_settings_prompt_english = "You are a helpful assistant."

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šé›†æˆå¤šç”¨æˆ·èŠå¤©æœåŠ¡ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from src.chatbot.services.multi_user_chat_service import (
    MultiUserChatService,
    ChatRequest, 
    ChatResponse,
    ChatServiceMode,
    create_multi_user_chat_service
)
from src.prompt.providers.prompt_provider import SecurityLevel

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ çŽ¯å¢ƒé…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
load_dotenv()
BASE_DIR = find_project_root()
DB_PATH = BASE_DIR / "src" / "runtime" / "chat" / "chat_memory.db"

OPENAI_BASE = os.getenv("OPENAI_BASE", "http://127.0.0.1:8080/v1")
OPENAI_KEY = os.getenv("OPENAI_KEY", "sk-fake-key")
MODEL_NAME = os.getenv("MODEL_NAME", "Gemma3")

USE_LTM = os.getenv("USE_LTM", "true").lower() not in {"0", "false", "no"}
LTM_SCORE_THRESHOLD = float(os.getenv("LTM_SCORE_THRESHOLD", 0.65))
LTM_MAX_HITS = int(os.getenv("LTM_MAX_HITS", 3))
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šæ”¯æŒç”¨æˆ·ç®¡ç†æ¨¡å¼ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
USE_USER_MANAGER = os.getenv("USE_USER_MANAGER", "true").lower() not in {"0", "false", "no"}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šæ”¯æŒå¤šç”¨æˆ·èŠå¤©æœåŠ¡æ¨¡å¼ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
USE_MULTI_USER_SERVICE = os.getenv("USE_MULTI_USER_SERVICE", "true").lower() not in {"0", "false", "no"}

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt / Regex â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
_LONG_PREFIX_HEADER = "ï¼ˆæˆ‘è®°å¾—è¿™äº›äº‹å¥½åƒåœ¨å“ªé‡Œå¬è¿‡ï¼Œä¹Ÿè®¸èƒ½ç”¨ä¸Š...ï¼‰\n"

_BOUNDARY_RE = re.compile(
    r"""
    [ã€‚ï¼ï¼Ÿï¼›!?]            |   # CJK å¥æœ«
    ([.!?])(?=\s|$)            # è‹±æ–‡å¥æœ«
    """,
    re.X,
)

_ABBR = {"Mr", "Mrs", "Ms", "Dr", "Prof", "Sr", "Jr", "St"}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç±»åž‹å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ChatMemory(TypedDict):
    conversation_history: List[BaseMessage]
    user_info: Dict[str, Any]
    last_interaction: str


class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_id: str
    language: str
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šå…¼å®¹æ—§ç‰ˆæœ¬è®°å¿†ç³»ç»Ÿ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    memory: Dict[str, ChatMemory]  # ä¿æŒå‘åŽå…¼å®¹


class ChatEngine:
    """LangGraph + TTSPlayer å®žæ—¶å¯¹è¯å¼•æ“Žï¼ˆçº¿ç¨‹/åç¨‹å®‰å…¨å•ä¾‹ï¼‰"""

    _instance: "ChatEngine | None" = None
    _instance_lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        raise RuntimeError("Use ChatEngine.get_instance() instead.")

    @classmethod
    def get_instance(cls, connect_to_unity: bool = False, use_user_manager: bool = None) -> "ChatEngine":
        with cls._instance_lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._init_once(
                    connect_to_unity=connect_to_unity,
                    use_user_manager=use_user_manager if use_user_manager is not None else USE_USER_MANAGER
                )
            return cls._instance

    # â”€â”€â”€â”€â”€ ç§æœ‰åˆå§‹åŒ– â”€â”€â”€â”€â”€
    def _init_once(self, connect_to_unity: bool = False, use_user_manager: bool = True):
        print(f"[ChatEngine] Initializing (connect_to_unity={connect_to_unity}, use_user_manager={use_user_manager})")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šå¤šç”¨æˆ·èŠå¤©æœåŠ¡é›†æˆ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.use_multi_user_service = USE_MULTI_USER_SERVICE
        if self.use_multi_user_service:
            self.multi_user_service = create_multi_user_chat_service(
                mode="hybrid",
                character_name="default",
                security_level="high"
            )
        else:
            self.multi_user_service = None

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šç”¨æˆ·ç®¡ç†å™¨é›†æˆ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.use_user_manager = use_user_manager
        if self.use_user_manager and not self.use_multi_user_service:
            # å¦‚æžœä½¿ç”¨å¤šç”¨æˆ·æœåŠ¡ï¼ŒUserManagerå·²ç»é›†æˆåœ¨æœåŠ¡ä¸­
            self.user_manager = UserManager()
        else:
            self.user_manager = None
            # ä¿æŒæ—§ç‰ˆæœ¬çš„å†…å­˜ç®¡ç†æ–¹å¼
            self._memory: Dict[str, ChatMemory] = {}
            self._memory_lock = threading.RLock()

        # TTS player & é˜Ÿåˆ—
        self._tts_player = TTSPlayer(TTSConfig(connect_to_unity=connect_to_unity))
        self._speak_q: "queue.Queue[str]" = __import__("queue").Queue()
        self._start_tts_thread()

        # LLM
        # self._llm = ChatOpenAI(
        #     openai_api_base=OPENAI_BASE,
        #     openai_api_key=OPENAI_KEY,
        #     model_name=MODEL_NAME,
        #     streaming=True,
        # )

        self._llm = ChatOpenAI(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            model_name="chatgpt-4o-latest",
            streaming=True,
        )

        # LTM
        self._ltm = (
            LongTermMemoryES(persist=True, threshold=LTM_SCORE_THRESHOLD)
            if USE_LTM and LongTermMemoryES
            else None
        )

        # Sync checkpointer
        self._sync_db_conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        try:
            self._sync_saver = SqliteSaver(self._sync_db_conn)
            print("[Checkpoint-sync] SQLite â†’", DB_PATH)
        except Exception as exc:  # pragma: no cover
            self._sync_saver = MemorySaver()
            if self._sync_db_conn: # Close if SqliteSaver failed
                self._sync_db_conn.close()
                self._sync_db_conn = None
            print("[Checkpoint-sync] MemorySaver â€”", exc)

        # Lazy-per-loop async graphs and their savers
        self._loop_graph_components: Dict[int, tuple[StateGraph, AsyncSqliteSaver, Any]] = {} # Stores (graph, saver, conn)
        self._loop_graphs_lock = threading.RLock()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ å…¬å…±åç¨‹å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def stream_chat(self, user_id: str, msg: str, language: str = "English") -> str:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šä½¿ç”¨å¤šç”¨æˆ·èŠå¤©æœåŠ¡ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.use_multi_user_service:
            return await self.stream_chat_multi_user(user_id, "", msg, language)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• åŽŸæœ‰é€»è¾‘ä¿æŒä¸å˜ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        graph, _, _ = await self._graph_for_loop() # We only need the graph here
        cfg = {"configurable": {"thread_id": f"persistent_{user_id}"}}
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šæ”¯æŒæ–°æ—§è®°å¿†ç³»ç»Ÿ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.use_user_manager:
            # ä½¿ç”¨æ–°çš„ç”¨æˆ·ç®¡ç†å™¨
            memory = {}  # ç©ºå­—å…¸ï¼Œå› ä¸ºè®°å¿†ç®¡ç†å·²è¿ç§»åˆ° UserManager
        else:
            # ä½¿ç”¨æ—§çš„å†…å­˜è®°å¿†ç³»ç»Ÿ
            memory = self._memory
            
        state = {
            "messages": [HumanMessage(content=msg)],
            "user_id": user_id,
            "language": language,
            "memory": memory,
        }
        result = await graph.ainvoke(state, cfg)
        return result["messages"][-1].content

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šå¤šç”¨æˆ·èŠå¤©æŽ¥å£ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    async def stream_chat_multi_user(
        self,
        user_id: str,
        username: str,
        message: str,
        language: str = "ä¸­æ–‡",
        security_level: SecurityLevel = SecurityLevel.HIGH,
        character_name: Optional[str] = None
    ) -> str:
        """ä½¿ç”¨æ–°çš„å¤šç”¨æˆ·èŠå¤©æœåŠ¡è¿›è¡Œå¯¹è¯"""
        if not self.use_multi_user_service or not self.multi_user_service:
            raise RuntimeError("MultiUserChatService is not enabled")
        
        # åˆ›å»ºèŠå¤©è¯·æ±‚
        request = ChatRequest(
            user_id=user_id,
            username=username or user_id,
            message=message,
            language=language,
            security_level=security_level,
            character_name=character_name
        )
        
        # å¤„ç†è¯·æ±‚
        response = await self.multi_user_service.process_chat_request(
            request,
            llm_callback=self._llm_callback_for_multi_user
        )
        
        # å¤„ç†TTS
        if response.response_message:
            sentences = self._split_into_sentences(response.response_message)
            for sentence in sentences:
                if sentence.strip():
                    self._speak_q.put(sentence.strip())
        
        return response.response_message

    async def _llm_callback_for_multi_user(
        self,
        chat_template: ChatPromptTemplate,
        history: List[BaseMessage],
        messages: List[BaseMessage]
    ) -> str:
        """å¤šç”¨æˆ·æœåŠ¡çš„LLMå›žè°ƒå‡½æ•°"""
        try:
            # æž„å»ºprompt
            prompt_obj = chat_template.invoke({
                "history": history,
                "messages": messages
            })

            # æµå¼ç”Ÿæˆ
            buf = ""
            out_tokens: List[str] = []
            final_content = ""

            async for chunk in self._llm.astream(prompt_obj):
                tok = chunk.content if isinstance(chunk, AIMessage) else chunk.get("content", "")
                print(tok, end="", flush=True)
                buf += tok
                out_tokens.append(tok)

                # å®žæ—¶TTSå¤„ç†ï¼ˆå¦‚æžœéœ€è¦ï¼‰
                while True:
                    m = _BOUNDARY_RE.search(buf)
                    if not m or self._is_ellipsis(buf, m.start()):
                        break
                    cut = m.end()
                    sent = buf[:cut].strip()
                    if sent:
                        # æ³¨æ„ï¼šè¿™é‡Œä¸ç›´æŽ¥æ”¾å…¥TTSé˜Ÿåˆ—ï¼Œè€Œæ˜¯åœ¨ä¸Šå±‚å¤„ç†
                        pass
                    buf = buf[cut:]

            if buf.strip():
                # åŒæ ·ï¼Œè¿™é‡Œä¸ç›´æŽ¥å¤„ç†TTS
                pass

            final_content = "".join(out_tokens)

            if not final_content.strip():
                final_content = "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„æ„æ€ï¼Œèƒ½å†è¯´ä¸€éå—ï¼Ÿ"

            return final_content

        except Exception as e:
            print(f"\n[ChatEngine _llm_callback_for_multi_user] Error: {e}")
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºçŽ°äº†é—®é¢˜ï¼Œè¯·ç¨åŽå†è¯•ã€‚"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šå¤šç”¨æˆ·æœåŠ¡ç®¡ç†æŽ¥å£ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def get_multi_user_service(self) -> MultiUserChatService:
        """èŽ·å–å¤šç”¨æˆ·èŠå¤©æœåŠ¡å®žä¾‹"""
        if not self.use_multi_user_service or not self.multi_user_service:
            raise RuntimeError("MultiUserChatService is not enabled")
        return self.multi_user_service

    async def get_user_stats(self) -> Dict[str, Any]:
        """èŽ·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯ï¼ˆé›†æˆç‰ˆæœ¬ï¼‰"""
        if self.use_multi_user_service and self.multi_user_service:
            service_stats = await self.multi_user_service.get_service_stats()
            return {
                "service_type": "multi_user_service",
                "stats": service_stats,
                "tts_queue_size": self._speak_q.qsize(),
                "tts_is_busy": self.tts_is_busy()
            }
        elif self.use_user_manager and self.user_manager:
            user_stats = self.user_manager.get_stats()
            return {
                "service_type": "user_manager",
                "stats": user_stats,
                "tts_queue_size": self._speak_q.qsize(),
                "tts_is_busy": self.tts_is_busy()
            }
        else:
            return {
                "service_type": "legacy",
                "memory_users": len(self._memory) if hasattr(self, '_memory') else 0,
                "tts_queue_size": self._speak_q.qsize(),
                "tts_is_busy": self.tts_is_busy()
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šé›†æˆæŽ¥å£æ–¹æ³• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def get_user_manager(self) -> UserManager:
        """èŽ·å–ç”¨æˆ·ç®¡ç†å™¨å®žä¾‹"""
        if not self.use_user_manager:
            raise RuntimeError("UserManager is not enabled for this ChatEngine instance")
        return self.user_manager

    def register_user(self, user_id: str, username: str = None, metadata: Dict[str, Any] = None):
        """æ³¨å†Œç”¨æˆ·ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰"""
        if self.use_user_manager:
            return self.user_manager.register_user(user_id, username, metadata)
        else:
            # æ—§ç‰ˆæœ¬å…¼å®¹
            with self._memory_lock:
                if user_id not in self._memory:
                    self._memory[user_id] = ChatMemory(conversation_history=[], user_info={}, last_interaction="")

    def add_message_to_memories(self, user_id: str, human_msg: BaseMessage, ai_msg: BaseMessage) -> None:
        """æ·»åŠ æ¶ˆæ¯åˆ°è®°å¿†ç³»ç»Ÿ"""
        if self.use_user_manager:
            # ä½¿ç”¨æ–°çš„ç”¨æˆ·ç®¡ç†å™¨
            self.user_manager.add_message_to_both(user_id, human_msg)
            self.user_manager.add_message_to_both(user_id, ai_msg)
        else:
            # ä½¿ç”¨æ—§çš„è®°å¿†ç³»ç»Ÿ
            with self._memory_lock:
                if user_id not in self._memory:
                    self._memory[user_id] = ChatMemory(conversation_history=[], user_info={}, last_interaction="")
                self._memory[user_id]["conversation_history"].extend([human_msg, ai_msg])
                self._memory[user_id]["last_interaction"] = datetime.now().isoformat()

    # â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å†…éƒ¨ï¼šTTS é˜Ÿåˆ—çº¿ç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    def _start_tts_thread(self):
        if hasattr(self, "_tts_thread") and self._tts_thread.is_alive():
            return

        def _worker():
            while True:
                part = self._speak_q.get()
                if part is None:
                    break
                try:
                    self._tts_player.stream(part)
                except Exception as exc:  # pragma: no cover
                    print(f"\n[TTS error] {exc}\n")

        self._tts_thread = threading.Thread(target=_worker, daemon=True)
        self._tts_thread.start()

    def _retrieve_memory(self, state: ChatState) -> ChatState:
        uid = state["user_id"]
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šæ”¯æŒæ–°æ—§è®°å¿†ç³»ç»Ÿ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.use_user_manager:
            # ç¡®ä¿ç”¨æˆ·å·²æ³¨å†Œ
            self.user_manager.register_user(uid)
            self.user_manager.update_user_activity(uid)
        else:
            # ä½¿ç”¨æ—§çš„è®°å¿†ç³»ç»Ÿ
            with self._memory_lock:
                if uid not in self._memory:
                    self._memory[uid] = ChatMemory(conversation_history=[], user_info={}, last_interaction="")
                self._memory[uid]["last_interaction"] = datetime.now().isoformat()
                
        return state

    @staticmethod
    def _process_message(state: ChatState) -> ChatState:
        uid = state["user_id"]
        txt = state["messages"][-1].content.lower()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šè¿™ä¸ªæ–¹æ³•åœ¨æ–°ç³»ç»Ÿä¸­å¯èƒ½éœ€è¦é‡æž„ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ç®€å•çš„åå­—æå–é€»è¾‘ä¿æŒä¸å˜ï¼Œä½†å­˜å‚¨æ–¹å¼éœ€è¦é€‚é…
        if "my name is" in txt:
            name = txt.split("my name is", 1)[1].strip().split()[0].capitalize()
            # åœ¨æ–°ç³»ç»Ÿä¸­ï¼Œè¿™ä¸ªä¿¡æ¯ä¼šé€šè¿‡ UserManager å­˜å‚¨
            if hasattr(state, 'memory') and uid in state["memory"]:
                state["memory"][uid]["user_info"]["name"] = name
                
        return state

    async def _gen_response(self, state: ChatState) -> ChatState:  # noqa: C901
        uid = state["user_id"]
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šè®°å¿†èŽ·å–é€»è¾‘ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.use_user_manager:
            # ä½¿ç”¨æ–°çš„ç”¨æˆ·ç®¡ç†å™¨èŽ·å–è®°å¿†
            personal_memory = self.user_manager.get_personal_memory(uid)
            general_memory = self.user_manager.get_general_memory()
            conversation_history = personal_memory.get_recent_messages(limit=50) if personal_memory else []
        else:
            # ä½¿ç”¨æ—§çš„è®°å¿†ç³»ç»Ÿ
            mem = state["memory"][uid]
            conversation_history = mem["conversation_history"]

        # ---------- LTM ----------
        prefix = ""
        if self._ltm and state["messages"]:
            q = state["messages"][-1].content
            docs = self._ltm.retrieve(q, k=LTM_MAX_HITS)
            if docs:
                body = "\n\n".join(f"[{d['score']:.2f}] {d['content']}" for d in docs)
                prefix = f"{_LONG_PREFIX_HEADER}{body}\n\n"

        prompt_obj = ChatPromptTemplate.from_messages(
            [
                ("system", f"{prefix}{general_settings_prompt_english}"),
                MessagesPlaceholder("history"),
                MessagesPlaceholder("messages"),
            ]
        ).invoke(
            {"history": conversation_history, "messages": state["messages"]}
        )

        buf = ""
        out_tokens: List[str] = []
        final_content = ""  # Initialize final_content

        try:
            async for chunk in self._llm.astream(prompt_obj):
                tok = chunk.content if isinstance(chunk, AIMessage) else chunk.get("content", "")
                print(tok, end="", flush=True)
                buf += tok
                out_tokens.append(tok)

                while True:
                    m = _BOUNDARY_RE.search(buf)
                    if not m or self._is_ellipsis(buf, m.start()):
                        break
                    cut = m.end()
                    sent = buf[:cut].strip()
                    if sent:
                        self._speak_q.put(sent)
                    buf = buf[cut:]
            
            if buf.strip():
                self._speak_q.put(buf.strip())
            
            final_content = "".join(out_tokens)

            # If after a successful stream, the content is empty/whitespace, provide a fallback.
            if not final_content.strip():
                print("\n[ChatEngine] LLM stream completed but resulted in empty/whitespace content. Using fallback.")
                final_content = "I'm sorry, I didn't quite understand. Could you say that again?"
                self._speak_q.put(final_content) # Send fallback to TTS

        except Exception as e:
            # Catch any exception during the streaming process
            print(f"\n[ChatEngine _gen_response] Error during LLM stream: {type(e).__name__}: {e}")
            # Provide a fallback response if an error occurs
            final_content = "I encountered an issue while processing your request. Please try again."
            self._speak_q.put(final_content) # Send fallback to TTS
            # The exception 'e' is not re-raised, allowing graph execution to continue with fallback content.

        ai_msg = AIMessage(content=final_content)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ä¿®æ”¹ï¼šæ¶ˆæ¯ä¿å­˜é€»è¾‘ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.use_user_manager:
            # ä½¿ç”¨æ–°çš„ç”¨æˆ·ç®¡ç†å™¨ä¿å­˜æ¶ˆæ¯
            human_msg = state["messages"][-1]  # å½“å‰çš„äººç±»æ¶ˆæ¯
            self.user_manager.add_message_to_both(uid, human_msg)
            self.user_manager.add_message_to_both(uid, ai_msg)
        else:
            # ä½¿ç”¨æ—§çš„è®°å¿†ç³»ç»Ÿ
            mem = state["memory"][uid]
            mem["conversation_history"].extend(state["messages"])
            mem["conversation_history"].append(ai_msg)
            
        return {"messages": [ai_msg]}

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Graph cache per-loop â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def _graph_for_loop(self) -> tuple[StateGraph, AsyncSqliteSaver, Any]: # Return tuple
        loop_id = id(asyncio.get_running_loop())
        with self._loop_graphs_lock:
            if loop_id in self._loop_graph_components:
                return self._loop_graph_components[loop_id]

        import aiosqlite # Import here as it's an async context

        conn = None
        saver = None
        try:
            conn = await aiosqlite.connect(DB_PATH)
            saver = AsyncSqliteSaver(conn)
            print(f"[Checkpoint-async] SQLite aio â†’ {DB_PATH} (loop {loop_id})")
        except Exception as exc:  # pragma: no cover
            if conn: # Close connection if AsyncSqliteSaver failed
                await conn.close()
            conn = None
            saver = MemorySaver() # Fallback to MemorySaver
            print("[Checkpoint-async] MemorySaver â€”", exc)

        g = StateGraph(state_schema=ChatState)
        g.add_node("retrieve_memory", self._retrieve_memory)
        g.add_node("process_message", self._process_message)
        g.add_node("generate_response", self._gen_response)
        g.add_edge(START, "retrieve_memory")
        g.add_edge("retrieve_memory", "process_message")
        g.add_edge("process_message", "generate_response")
        g.add_edge("generate_response", END)
        graph = g.compile(checkpointer=saver)

        with self._loop_graphs_lock:
            # Store graph, saver, and connection for later cleanup
            self._loop_graph_components[loop_id] = (graph, saver, conn)
            return graph, saver, conn

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _is_ellipsis(txt: str, idx: int) -> bool:
        return txt[idx] == "." and idx >= 2 and txt[idx - 2: idx + 1] == "..."

    async def close(self): # Make close async
        self._speak_q.put(None)
        if hasattr(self, "_tts_thread"):
            self._tts_thread.join()
        self._tts_player.close()

        # Close synchronous checkpointer connection
        if isinstance(self._sync_saver, SqliteSaver) and self._sync_db_conn:
            self._sync_db_conn.close()
            print("[Checkpoint-sync] SQLite connection closed.")
        
        # Close asynchronous checkpointer connections
        with self._loop_graphs_lock:
            for graph, saver, conn in self._loop_graph_components.values():
                if isinstance(saver, AsyncSqliteSaver) and conn:
                    try:
                        await conn.close() # Close the aiosqlite connection directly
                        print(f"[Checkpoint-async] SQLite aio connection closed for loop {id(graph)}.")
                    except Exception as e:
                        print(f"[Checkpoint-async] Error closing SQLite aio connection: {e}")
            self._loop_graph_components.clear()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• æ–°å¢žï¼šå…³é—­ç”¨æˆ·ç®¡ç†å™¨ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.use_user_manager and self.user_manager:
            await self.user_manager.close()

    def tts_is_busy(self) -> bool:
        return self._tts_player.is_busy()

    def speech_queue_empty(self) -> bool:
        return self._speak_q.empty()

    def is_speaking(self) -> bool:
        """æ£€æŸ¥ TTS æ˜¯å¦æ­£åœ¨è¯´è¯æˆ–é˜Ÿåˆ—ä¸­æ˜¯å¦æœ‰å¾…å¤„ç†çš„è¯­éŸ³ã€‚"""
        return not self._speak_q.empty() or self._tts_player.is_busy()


# cli quick test
async def _cli():
    eng = ChatEngine.get_instance(connect_to_unity=False)
    print("ðŸ’¬  Real-time voice chat â€” è¾“å…¥æ–‡å­— (quit/exit é€€å‡º)\n")
    try:
        while True:
            inp = input("ðŸ‘¤ > ").strip()
            if inp.lower() in {"quit", "exit"}:
                break
            if not inp:
                continue
            await eng.stream_chat("demo_user60", inp, language="English")
            print()
    finally:
        await eng.close()


if __name__ == "__main__":
    asyncio.run(_cli())
