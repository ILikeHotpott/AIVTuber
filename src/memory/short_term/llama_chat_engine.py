#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
memory_chat_stream_tts.py â€” 2025-05-28 (strict-sentence-boundary)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ å®Œå…¨ä¾èµ–å¥æœ«æ ‡ç‚¹æ‰æ’­æŠ¥ï¼Œç»ä¸åœ¨å¥ä¸­æ–­å¥
â€¢ æ”¯æŒ Elasticsearch-based **é•¿è®°å¿†æ£€ç´¢**ï¼ˆ--no-ltm å…³é—­ï¼‰
â€¢ å…¶ä½™æµå¼åˆ†å¥ + TTS é€»è¾‘ä¸æ—§ç‰ˆä¸€è‡´
"""

from __future__ import annotations

import os, re, queue, sqlite3, threading, time, asyncio, argparse
from datetime import datetime
from typing import Annotated, Dict, List, Sequence, Any, TypedDict

from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langgraph.graph import START, END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

from src.prompt.templates.general import general_settings_prompt
from src.memory.long_term.elastic_search import LongTermMemoryES
from src.tts.tts_stream import tts_streaming
from src.utils.path import find_project_root

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI & Config â”€â”€â”€â”€â”€â”€â”€â”€â”€
config = {"use_ltm": True}
parser = argparse.ArgumentParser(description="Realtime chat + TTS (with long-term memory)")
parser.add_argument("--no-ltm", action="store_true", help="disable Elasticsearch long-term memory")
args, _ = parser.parse_known_args()
if args.no_ltm:
    config["use_ltm"] = False

load_dotenv()
BASE_DIR = find_project_root()
DB_PATH = BASE_DIR / "src" / "runtime" / "chat" / "chat_memory.db"
USE_LONG_TERM = config.get("use_ltm", True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = ChatOpenAI(
    openai_api_base="http://127.0.0.1:8080/v1",
    openai_api_key="sk-fake-key",
    model_name="llama",
    streaming=True,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ é•¿è®°å¿†æ£€ç´¢å™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€
_long_prefix_header = "ï¼ˆæˆ‘è®°å¾—è¿™äº›äº‹å¥½åƒåœ¨å“ªé‡Œå¬è¿‡ï¼Œä¹Ÿè®¸èƒ½ç”¨ä¸Š...ï¼‰\n"
ltm = (
    LongTermMemoryES(persist=True, threshold=float(os.getenv("LTM_SCORE_THRESHOLD", 0.65)))
    if USE_LONG_TERM
    else None
)
MAX_HITS = int(os.getenv("LTM_MAX_HITS", 3))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç±»å‹å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ChatMemory(TypedDict):
    conversation_history: List[BaseMessage]
    user_info: Dict[str, Any]
    last_interaction: str


class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_id: str
    language: str
    memory: Dict[str, ChatMemory]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Checkpointer (sync) â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    _sync_conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    _sync_saver = SqliteSaver(_sync_conn)
    print("[Checkpoint-sync] SQLite ->", DB_PATH)
except Exception as e:
    print("[Checkpoint-sync] fallback MemorySaver â€”", e)
    _sync_saver = MemorySaver()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt builder helper â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_prompt_template(memory_prefix: str = "") -> ChatPromptTemplate:
    system_text = f"{memory_prefix}{general_settings_prompt}"
    return ChatPromptTemplate.from_messages(
        [
            ("system", system_text),
            MessagesPlaceholder("history"),
            MessagesPlaceholder("messages"),
        ]
    )


_BOUNDARY_RE = re.compile(
    r"""
    [ã€‚ï¼ï¼Ÿï¼›]                              # ä¸­æ–‡å¥æœ«
    |
    ([.!?])(?=\s|$)                         # è‹±æ–‡å¥æœ«å€™é€‰ï¼ˆä½†ä¸æ’é™¤ç¼©å†™ï¼‰
    """,
    re.X
)

# è‹±æ–‡ç¼©å†™åˆ—è¡¨ï¼ˆç”¨äºåå¤„ç†ï¼‰
_ABBR = {"Mr", "Mrs", "Ms", "Dr", "Prof", "Sr", "Jr", "St"}

_speak_q: queue.Queue[str] = queue.Queue()


def _is_ellipsis(txt: str, idx: int) -> bool:
    return txt[idx] == "." and idx >= 2 and txt[idx - 2: idx + 1] == "..."


def _tts_worker():
    while True:
        part = _speak_q.get()
        if part is None:
            break
        try:
            tts_streaming(part)
        except Exception as e:
            print(f"\n[TTS error] {e}\n")


_tts_thread = threading.Thread(target=_tts_worker, daemon=True)
_tts_thread.start()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ LangGraph èŠ‚ç‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _retrieve_memory(state: ChatState) -> ChatState:
    uid = state["user_id"]
    mem = state.setdefault("memory", {})
    if uid not in mem:
        mem[uid] = ChatMemory(conversation_history=[], user_info={}, last_interaction="")
    mem[uid]["last_interaction"] = datetime.now().isoformat()
    return state


def _process_message(state: ChatState) -> ChatState:
    uid = state["user_id"]
    txt = state["messages"][-1].content.lower()
    if "my name is" in txt:
        name = txt.split("my name is", 1)[1].strip().split()[0].capitalize()
        state["memory"][uid]["user_info"]["name"] = name
    return state


async def _generate_response_stream(state: ChatState) -> ChatState:
    uid = state["user_id"]
    mem = state["memory"][uid]

    # ----- long-term memory -----
    memory_prefix = ""
    if ltm and state["messages"]:
        query = state["messages"][-1].content
        docs = ltm.retrieve(query, k=MAX_HITS)
        if docs:
            body = "\n\n".join(f"[{d['score']:.2f}] {d['content']}" for d in docs)
            memory_prefix = f"{_long_prefix_header}{body}\n\n"

    prompt_tmpl = build_prompt_template(memory_prefix)
    prompt = prompt_tmpl.invoke(
        {
            "history": mem["conversation_history"],
            "messages": state["messages"],
        }
    )

    buf = ""
    tokens = []

    async for chunk in model.astream(prompt):
        tok = chunk.content if isinstance(chunk, AIMessage) else chunk.get("content", "")
        print(tok, end="", flush=True)
        buf += tok
        tokens.append(tok)

        # åªåœ¨çœŸæ­£å¥æœ«æ ‡ç‚¹å¤„åˆ†å¥
        while True:
            m = _BOUNDARY_RE.search(buf)
            if not m or _is_ellipsis(buf, m.start()):
                break
            cut = m.end()
            sent = buf[:cut].strip()
            if sent:
                _speak_q.put(sent)
            buf = buf[cut:]

    # flush ä½™ä¸‹æ®‹å¥
    if buf.strip():
        _speak_q.put(buf.strip())

    full_msg = AIMessage(content="".join(tokens))
    mem["conversation_history"].extend(state["messages"])
    mem["conversation_history"].append(full_msg)
    return {"messages": [full_msg]}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Build graph helper â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _build_graph(checkpointer):
    g = StateGraph(state_schema=ChatState)
    g.add_node("retrieve_memory", _retrieve_memory)
    g.add_node("process_message", _process_message)
    g.add_node("generate_response", _generate_response_stream)
    g.add_edge(START, "retrieve_memory")
    g.add_edge("retrieve_memory", "process_message")
    g.add_edge("process_message", "generate_response")
    g.add_edge("generate_response", END)
    return g.compile(checkpointer=checkpointer)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Async graph cache per-loop â”€â”€â”€â”€â”€â”€â”€â”€â”€
_graph_cache: Dict[int, StateGraph] = {}


async def _graph_for_current_loop() -> StateGraph:
    loop_id = id(asyncio.get_running_loop())
    if loop_id in _graph_cache:
        return _graph_cache[loop_id]

    import aiosqlite

    try:
        conn = await aiosqlite.connect(DB_PATH)
        saver = AsyncSqliteSaver(conn)
        print(f"[Checkpoint-async] SQLite aio -> {DB_PATH} (loop {loop_id})")
    except Exception as e:
        print("[Checkpoint-async] fallback MemorySaver â€”", e)
        saver = MemorySaver()

    g = _build_graph(saver)
    _graph_cache[loop_id] = g
    return g


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Public entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def stream_chat(user_id: str, msg: str, language: str = "English") -> str:
    graph = await _graph_for_current_loop()
    cfg = {"configurable": {"thread_id": f"persistent_{user_id}"}}
    state = {
        "messages": [HumanMessage(content=msg)],
        "user_id": user_id,
        "language": language,
        "memory": {},
    }
    result = await graph.ainvoke(state, cfg)
    return result["messages"][-1].content


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI interface â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    print("ğŸ’¬ Real-time voice chat â€” type something (type 'quit' to exit)\n")
    try:
        while True:
            inp = input("ğŸ‘¤ > ").strip()
            if inp.lower() in {"quit", "exit"}:
                break
            if not inp:
                continue
            await stream_chat("demo_user3", inp, language="Chinese")
            print()
    finally:
        # ç¡®ä¿ TTS çº¿ç¨‹ä¼˜é›…é€€å‡º
        _speak_q.put(None)
        _tts_thread.join()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Entrypoint â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    asyncio.run(main())
