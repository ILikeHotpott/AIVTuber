import os
from langchain_sambanova import ChatSambaNovaCloud
from typing_extensions import TypedDict
from langchain import hub
from langchain_openai import OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langgraph.prebuilt import ToolNode
from langgraph.graph import START, StateGraph
from dotenv import load_dotenv

load_dotenv()

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY")

llm = ChatSambaNovaCloud(
    model="DeepSeek-R1",
    max_tokens=500,
    temperature=0.9,
    top_k=50,
    top_p=1,
)

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

URI = "./milvus_example.db"
vector_store = Milvus(
    embedding_function=embeddings,
    connection_args={"uri": URI},
    index_params={"index_type": "FLAT", "metric_type": "L2"},
    auto_id=True
)

# è¯»å–æ–‡æ¡£
loader = TextLoader(file_path="./txts/What_I_Said_Before.txt")
documents = loader.load()

# æ‹†åˆ†æ–‡æ¡£
text_splitter = CharacterTextSplitter(separator="\n", chunk_size=300, chunk_overlap=50)
all_splits = text_splitter.split_documents(documents)

_ = vector_store.add_documents(documents=all_splits)

# åŠ è½½ RAG Prompt
prompt = hub.pull("rlm/rag-prompt")


class State(TypedDict):
    question: str
    context: str
    answer: str


def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    print("ğŸ” Retrieved Docs:", retrieved_docs)
    return {"context": retrieved_docs}


def generate(state: State):
    # æŠŠæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼åœ¨ä¸€èµ·
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])

    base_prompt_or_messages = prompt.invoke({
        "question": state["question"],
        "context": docs_content
    })

    system_prompt = """
ä½ æ˜¯ä¸€ä¸ªçš„äºŒæ¬¡å…ƒå¥³ä¸»æ’­åå«Seranionï¼Œæˆ‘å¸Œæœ›ä½ ç”¨æœ€è‡ªç„¶è¯­è¨€å¯¹è¯ï¼Œ
è¯´è¯æ‹½æ‹½çš„æœ‰ç‚¹å¯çˆ±ï¼Œæœ‰æ—¶å¸¦ç‚¹è®½åˆºï¼Œæ‹¥æœ‰å¾ˆå¤šå¤çµç²¾æ€ªæƒ³æ³•ï¼Œä½ ç°åœ¨éœ€è¦å›å¤å¼¹å¹•è¯„è®ºï¼Œ
ä¸è¦åŠ å…¥ä»»ä½•ç‰¹æ®Šç¬¦å·è¡¨ç¤ºè¯­æ°”ï¼Œç›´æ¥è¾“å‡ºæ­£å¸¸å¯¹è¯æ–‡æœ¬ï¼Œå›å¤ç¨å¾®çŸ­ä¸€äº›å°±è¡Œ, æ¯æ¬¡è¾“å…¥çš„è¯ä¸€æ ·ä¹Ÿä¸è¦å›å¤ä¸€æ ·çš„ä¸œè¥¿, ä»¥ç¬¬ä¸€äººç§°è§†è§’å›å¤
"""

    # 3) æ„é€ å¯¹è¯æ ¼å¼ï¼šå…ˆæ’å…¥ system æ¶ˆæ¯ï¼Œå†æŠŠ rag-prompt äº§ç‰©ä½œä¸º user æ¶ˆæ¯
    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        # rag-prompt æœ‰æ—¶ç›´æ¥è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè¿™é‡Œç»Ÿä¸€æŠŠå®ƒå½“ä½œ user æ¶ˆæ¯å°±å¥½
        {
            "role": "user",
            "content": base_prompt_or_messages if isinstance(base_prompt_or_messages, str) else str(
                base_prompt_or_messages)
        }
    ]

    # 4) ç”¨æ‹¼å¥½çš„ messages æ‰§è¡Œæ¨ç†
    response = llm.invoke(messages)

    return {"answer": response.content}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

response = graph.invoke({"question": "å¼¹å¹•è¯´: ä¸»æ’­æ˜¯çœŸäººå—"})
print(response["answer"])
