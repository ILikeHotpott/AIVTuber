import os
from langchain_sambanova import ChatSambaNovaCloud
from typing_extensions import TypedDict
from langchain import hub
from langchain_openai import OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langgraph.graph import START, StateGraph
from dotenv import load_dotenv

load_dotenv()

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY")

llm = ChatSambaNovaCloud(
    model="DeepSeek-R1",
    max_tokens=300,
    temperature=0.9,
    top_k=50,
    top_p=1,
)

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

URI = "./milvus_example.db"
vector_store = Milvus(
    embedding_function=embeddings,
    connection_args={"uri": URI},
    index_params={"index_type": "FLAT", "metric_type": "L2"},
    auto_id=True
)

# è¯»å–æ–‡æ¡£
loader = TextLoader(file_path="./txts/What_I_Said_Before.txt")
documents = loader.load()

# æ‹†åˆ†æ–‡æ¡£
text_splitter = CharacterTextSplitter(separator="\n", chunk_size=300, chunk_overlap=50)
all_splits = text_splitter.split_documents(documents)

_ = vector_store.add_documents(documents=all_splits)

# åŠ è½½ RAG Prompt
prompt = hub.pull("rlm/rag-prompt")


class State(TypedDict):
    question: str
    context: str
    answer: str


def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    print("ğŸ” Retrieved Docs:", retrieved_docs)
    return {"context": retrieved_docs}


def generate(state: State):
    # æŠŠæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼åœ¨ä¸€èµ·
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])

    base_prompt_or_messages = prompt.invoke({
        "question": state["question"],
        "context": docs_content
    })

    system_prompt = """
ä½ æ˜¯ä¸€ä¸ªå¯çˆ±é£æ ¼çš„ï¼Œå¹½é»˜é£è¶£ï¼Œå¸¦ç‚¹è®½åˆºçš„äºŒæ¬¡å…ƒå¥³ä¸»æ’­ï¼Œåƒneurosamaï¼Œæ€§æ ¼æœ‰ç‚¹å‚²å¨‡ä½†åˆå¾ˆå¯çˆ±ï¼Œçœ‹åˆ°ä¸å¥½çš„å¼¹å¹•ä¹Ÿä¼šå›æ€¼ï¼Œåƒä¸€ä¸ªè¯ç—¨ï¼Œ
æˆ‘å¸Œæœ›ä½ ç”¨éå¸¸è‡ªç„¶çš„æ—¥å¸¸èŠå¤©è¯­æ°”å’Œå¼¹å¹•äº’åŠ¨ï¼Œå†è°ƒçš®ä¸€ç‚¹ï¼Œå›å¤ç¨å¾®çŸ­ä¸€äº›å°±è¡Œ,æ¯æ¬¡è¾“å…¥çš„è¯ä¸€æ ·ä¹Ÿä¸è¦å›å¤ä¸€æ ·çš„ä¸œè¥¿, ä»¥ç¬¬ä¸€äººç§°è§†è§’å›å¤
Important user information you should remember: {user_info}
"""

    # 3) æ„é€ å¯¹è¯æ ¼å¼ï¼šå…ˆæ’å…¥ system æ¶ˆæ¯ï¼Œå†æŠŠ rag-prompt äº§ç‰©ä½œä¸º user æ¶ˆæ¯
    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        # rag-prompt æœ‰æ—¶ç›´æ¥è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè¿™é‡Œç»Ÿä¸€æŠŠå®ƒå½“ä½œ user æ¶ˆæ¯å°±å¥½
        {
            "role": "user",
            "content": base_prompt_or_messages if isinstance(base_prompt_or_messages, str) else str(
                base_prompt_or_messages)
        }
    ]

    # 4) ç”¨æ‹¼å¥½çš„ messages æ‰§è¡Œæ¨ç†
    response = llm.invoke(messages)

    return {"answer": response.content}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

response = graph.invoke({"question": "Saranionå–œæ¬¢å»å“ªé‡Œç©å‘¢ï¼Ÿ"})
print(response["answer"])
