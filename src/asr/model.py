# --- dependencies ---
# pip install sounddevice soundfile numpy webrtcvad lightning-whisper-mlx

from __future__ import annotations
import threading, time, tempfile, collections, itertools
from typing import Iterable, Callable, Optional, Deque

import numpy as np, sounddevice as sd, soundfile as sf, webrtcvad
from lightning_whisper_mlx import LightningWhisperMLX

# ---------- ÂèÇÊï∞ ----------
SAMPLE_RATE = 16_000
FRAME_DURATION_MS = 30  # WebRTC VAD ÊîØÊåÅ 10 / 20 / 30
FRAME_SIZE = SAMPLE_RATE * FRAME_DURATION_MS // 1000  # samples / frame
MAX_SILENCE_FRAMES = int(0.6 * 1000 / FRAME_DURATION_MS)  # 0.6s
MIN_SPEECH_FRAMES = int(0.3 * 1000 / FRAME_DURATION_MS)  # 0.3s
VAD_SENSITIVITY = 2  # 0=ÊúÄÂÆΩÊùæ, 3=ÊúÄ‰∏•Ê†º
DEBUG = False

_model: LightningWhisperMLX | None = None


def _lazy_model() -> LightningWhisperMLX:
    global _model
    if _model is None:
        print("‚åõ Ê≠£Âú®Âä†ËΩΩ Whisper-MLX‚Ä¶")
        _model = LightningWhisperMLX(model="small", batch_size=12)
        print("‚úÖ Whisper-MLX ready")
    return _model


def stream_transcripts(
        pause_event: threading.Event,
        on_partial: Optional[Callable[[], None]] = None,
) -> Iterable[str]:
    """
    + pause_event.clear()  ‚Üí ÈùôÈü≥È∫¶
    + on_partial()         ‚Üí Á¨¨‰∏ÄÂ∏ßËØ≠Èü≥Á´ãÂàªË∞ÉÁî®ÔºàÂèØÁî®‰∫é stop-TTSÔºâ
    """
    whisper = _lazy_model()
    vad = webrtcvad.Vad(VAD_SENSITIVITY)

    def is_speech(frame: bytes) -> bool:
        return vad.is_speech(frame, SAMPLE_RATE)

    with sd.RawInputStream(
            samplerate=SAMPLE_RATE, channels=1, blocksize=FRAME_SIZE, dtype="int16"
    ) as stream:
        buf: Deque[bytes] = collections.deque()
        silence = 0
        speech = 0
        spoke = False

        while True:
            if not pause_event.is_set():  # È∫¶ÂÖãÈ£éË¢´ÈùôÈü≥
                stream.read(FRAME_SIZE)
                continue

            data, _ = stream.read(FRAME_SIZE)
            if len(data) == 0:
                continue

            talking = is_speech(data)

            if DEBUG:
                print("üó£Ô∏è" if talking else "¬∑", end="", flush=True)

            if talking:
                if not spoke and on_partial:
                    on_partial()  # Á¨¨‰∏ÄÊ¨°Ê£ÄÊµãÂà∞ËØ≠Èü≥
                buf.append(data)
                silence = 0
                speech += 1
                spoke = True
            else:
                if spoke:
                    silence += 1

            # ËØ≠Èü≥ÁªìÊùüÔºöÁ¥ØÁßØÈùôÈªò > MAX_SILENCE_FRAMES
            if spoke and silence >= MAX_SILENCE_FRAMES:
                if speech >= MIN_SPEECH_FRAMES:
                    pcm = b"".join(buf)
                    wav = np.frombuffer(pcm, np.int16).astype(np.float32) / 32768
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                        sf.write(tmp.name, wav, SAMPLE_RATE, subtype="PCM_16")
                        t0 = time.perf_counter()
                        text = whisper.transcribe(audio_path=tmp.name)["text"].strip()
                        print(f"\n[Whisper] ‚è± {(time.perf_counter() - t0):.2f}s -> {text}")
                        yield text
                # reset
                buf.clear()
                silence = speech = 0
                spoke = False
                if DEBUG: print()  # Êç¢Ë°åÊòæÁ§∫
